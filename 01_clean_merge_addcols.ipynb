{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46060ca",
   "metadata": {},
   "source": [
    "\n",
    "# Riyadh Stations — Merge & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, pandas as pd, numpy as np\n",
    "\n",
    "CLEANED_PATH = \"cleaned_dataset.csv\"\n",
    "STATIONS_PATH = \"Riyadh_Stations_TwoU.csv\"\n",
    "\n",
    "def find_file(keyword, folder=\".\"):\n",
    "    for f in os.listdir(folder):\n",
    "        if keyword.lower() in f.lower():\n",
    "            return os.path.join(folder, f)\n",
    "    return None\n",
    "\n",
    "def resolve(p):\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "    alt = find_file(os.path.splitext(os.path.basename(p))[0], \"/mnt/data\")\n",
    "    if alt:\n",
    "        return alt\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "def read_csv_any(path):\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "        except Exception:\n",
    "            return pd.read_csv(path, sep=\";\", encoding=\"utf-8-sig\")\n",
    "\n",
    "df1 = read_csv_any(resolve(CLEANED_PATH))\n",
    "df2 = read_csv_any(resolve(STATIONS_PATH))\n",
    "\n",
    "df1.head(2), df2.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d528894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    s = s.replace(\"defence\", \"defense\")\n",
    "    s = s.replace(\"fahd\", \"fahad\")\n",
    "    return s\n",
    "\n",
    "df1[\"_k\"] = df1[\"Name\"].map(norm)\n",
    "df2[\"_k\"] = df2[\"Station name\"].map(norm)\n",
    "\n",
    "ALIAS = {\n",
    "    \"pnu 1\": \"pnu1\",\n",
    "    \"pnu1\": \"pnu1\",\n",
    "    \"pnu 2\": \"pnu2\",\n",
    "    \"pnu2\": \"pnu2\",\n",
    "    \"princess noura university 1\": \"pnu1\",\n",
    "    \"princess noura university 2\": \"pnu2\",\n",
    "    \"metro station a1\": \"pnu1\",\n",
    "    \"metro station a2\": \"pnu1\",\n",
    "    \"metro station a10\": \"pnu1\",\n",
    "    \"metro station f1\": \"pnu1\",\n",
    "    \"metro station s1\": \"pnu2\",\n",
    "    \"metro station 4\": \"pnu2\",\n",
    "    \"metro station a7\": \"pnu2\",\n",
    "    \"metro station a6\": \"pnu2\",\n",
    "    \"a5\": \"pnu2\",\n",
    "    \"ministry of defence\": \"ministry of defense\",\n",
    "    \"subway\": \"subway\",\n",
    "    \"subway  صب واي\": \"subway\",\n",
    "    \"subway salman al farsi\": \"subway\",\n",
    "}\n",
    "\n",
    "apply_alias = lambda k: ALIAS.get(k, k)\n",
    "\n",
    "df1[\"_k2\"] = df1[\"_k\"].apply(apply_alias)\n",
    "df2[\"_k2\"] = df2[\"_k\"].apply(apply_alias)\n",
    "\n",
    "df2u = df2.drop_duplicates(subset=[\"_k2\"], keep=\"first\").copy()\n",
    "\n",
    "cols_to_add = [\"Metro line number\", \"Metro line name\", \"Station type\"]\n",
    "df_merged = df1.merge(df2u[[\"_k2\"] + cols_to_add], on=\"_k2\", how=\"left\").drop(columns=[\"_k\",\"_k2\"])\n",
    "\n",
    "def canonical_name(original: str) -> str:\n",
    "    k = norm(original)\n",
    "    k2 = apply_alias(k)\n",
    "    if k2 == \"pnu1\":\n",
    "        return \"PNU1\"\n",
    "    if k2 == \"pnu2\":\n",
    "        return \"PNU2\"\n",
    "    return original\n",
    "\n",
    "df_merged[\"CanonicalName\"] = df_merged[\"Name\"].apply(canonical_name)\n",
    "\n",
    "# Consolidate duplicates with weighted ratings\n",
    "g = df_merged.groupby(\"CanonicalName\", dropna=False)\n",
    "tmp = g.agg({\n",
    "    \"Type_of_Utility\": lambda s: s.dropna().mode().iloc[0] if not s.dropna().mode().empty else s.dropna().iloc[0] if s.dropna().shape[0] else np.nan,\n",
    "    \"Number_of_Ratings\": \"sum\",\n",
    "    \"Rating\": \"mean\",\n",
    "    \"Longitude\": \"mean\",\n",
    "    \"Latitude\": \"mean\",\n",
    "    \"Metro line number\": lambda s: s.dropna().iloc[0] if s.dropna().shape[0] else np.nan,\n",
    "    \"Metro line name\": lambda s: s.dropna().iloc[0] if s.dropna().shape[0] else np.nan,\n",
    "    \"Station type\": lambda s: s.dropna().iloc[0] if s.dropna().shape[0] else np.nan,\n",
    "}).reset_index()\n",
    "\n",
    "weighted = (\n",
    "    df_merged\n",
    "    .assign(_val=pd.to_numeric(df_merged[\"Rating\"], errors=\"coerce\"),\n",
    "            _w=pd.to_numeric(df_merged[\"Number_of_Ratings\"], errors=\"coerce\").fillna(0))\n",
    "    .groupby(\"CanonicalName\")\n",
    "    .apply(lambda d: (d[\"_val\"].fillna(0) * d[\"_w\"]).sum() / d[\"_w\"].sum() if d[\"_w\"].sum() > 0 else d[\"_val\"].mean())\n",
    "    .rename(\"Rating\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_final = tmp.drop(columns=[\"Rating\"]).merge(weighted, on=\"CanonicalName\", how=\"left\")\n",
    "\n",
    "# Drop obvious empty rows\n",
    "key_numeric = [\"Rating\", \"Number_of_Ratings\", \"Longitude\", \"Latitude\"]\n",
    "df_final = df_final[~df_final[\"CanonicalName\"].isna()]\n",
    "df_final = df_final[~df_final[key_numeric].isna().all(axis=1)].reset_index(drop=True)\n",
    "\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUT_CSV = \"final_dataset_cleaned_v2.csv\"\n",
    "df_final.to_csv(OUT_CSV, index=False)\n",
    "OUT_CSV, df_final[df_final[\"CanonicalName\"].str.contains(\"PNU\", na=False)][[\"CanonicalName\",\"Metro line number\",\"Metro line name\",\"Station type\",\"Number_of_Ratings\",\"Rating\"]]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
